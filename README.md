# ID2223 Lab2 Group 2333

### Link to Hugging Face: https://huggingface.co/spaces/Yuxin020807/Iris

## Task 1
Fine-Tune a pre-trained large language (transformer) model and build a serverless UI for using that model

a. Fine-tune an existing pre-trained large language model on the FineTome Instruction Dataset

b. Build and run an inference pipeline with a Gradio UI on Hugging Face Spaces for your model.

## Task 2
Describe in your README.md program ways in which you can improve model performance using

(a) model-centric approach - e.g., tune hyperparameters, change the fine-tuning model architecture, etc

(b) data-centric approach - identify new data sources that enable you to train a better model that one provided in the blog post

1. If you can show results of improvement, then you get the top grade.
2. Try out fine-tuning a couple of different open-source foundation LLMs to get one that works best with your UI for inference (inference will be on CPUs, so big models will be slow).
3. You are free to use other fine-tuning frameworks, such as Axolotl of HF FineTuning - you do not have to use the provided unsloth notebook.

# Ways in which one can improve model performance
### Model-Centric Approach
- **Hyper Parameters Tunning:**
- **Choosing the Right Model:**
- **Using Pre-trained Models:**
### Data-Centric Approach
- **Selecting Dataset:**
- **Introducing Training and Validation Set:**
